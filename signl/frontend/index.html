<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Synapse - Real-Time AI</title>
    <style>
        :root {
            --panel-max-w: 400px;
            --panel-aspect: 4 / 3;
            --box-pad: 15px;
            --gap: 20px;
        }
        body { 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            background: linear-gradient(135deg, #1a1a1a, #2d2d2d);
            color: #e0e0e0; 
            display: flex; 
            flex-direction: column; 
            align-items: center; 
            margin: 0; 
            padding: 20px; 
        }
        .container { 
            display: flex; 
            gap: var(--gap); 
            width: 100%;
            max-width: 1400px;
            align-items: start;
        }
        .video-container {
            display: flex;
            flex-direction: column;
            gap: var(--gap);
            flex: 0 0 400px;
        }
        .translation-container {
            flex: 1;
            display: flex;
            flex-direction: column;
            gap: var(--gap);
        }
        @media (max-width: 1100px) { 
            .container { flex-direction: column; }
            .video-container { flex: 1; width: 100%; }
        }
        .video-box { 
            border: 3px solid #555; 
            border-radius: 12px; 
            padding: var(--box-pad); 
            background: linear-gradient(145deg, #2a2a2a, #3a3a3a);
            position: relative; 
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
            width: 100%;
            max-width: var(--panel-max-w);
        }
    .video-box h2 { margin: 0 0 8px 0; font-weight: 600; }
    .media-stack { position: relative; width: 100%; }
    .media-stack img, .media-stack canvas { width: 100%; height: auto; aspect-ratio: var(--panel-aspect); display: block; }
        video, img, canvas { display: block; border-radius: 8px; }
        /* Make media fluid while preserving intrinsic draw resolution (640x480) */
        video, img { width: 100%; height: auto; aspect-ratio: var(--panel-aspect); }
        /* Overlay canvas fills the media area, pointer-events off for clicks below */
        #overlay-canvas { position: absolute; inset: 0; width: 100%; height: 100%; pointer-events: none; }
        #status { 
            margin-top: 20px; 
            font-weight: bold; 
            padding: 10px 20px;
            border-radius: 20px;
            background-color: #2a2a2a;
        }
        .controls { 
            display: flex; 
            align-items: center; 
            gap: 15px; 
            margin-bottom: 20px; 
            padding: 15px; 
            background: linear-gradient(145deg, #2a2a2a, #3a3a3a);
            border-radius: 12px; 
            box-shadow: 0 4px 8px rgba(0,0,0,0.2);
            width: 100%;
            max-width: 1400px;
        }
        .controls select, .controls button { 
            font-size: 14px; 
            padding: 8px 12px; 
            border-radius: 6px; 
            border: 2px solid #555; 
            background-color: #333; 
            color: #e0e0e0; 
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .controls button:hover { background-color: #444; border-color: #777; }
        .stats { 
            position: absolute; 
            top: 5px; 
            right: 5px; 
            background: rgba(0,0,0,0.7); 
            color: #00ff00; 
            padding: 5px 8px; 
            border-radius: 4px; 
            font-size: 11px; 
            font-family: monospace; 
        }
        .connected { color: #00ff00; }
        .disconnected { color: #ff4444; }
        .error { color: #ff8888; }
        .translation-box {
            border: 3px solid #555; 
            border-radius: 12px; 
            padding: var(--box-pad); 
            background: linear-gradient(145deg, #2a2a2a, #3a3a3a);
            box-shadow: 0 8px 16px rgba(0,0,0,0.3);
            height: 500px;
            overflow-y: auto;
        }
        .translation-item {
            padding: 10px;
            margin-bottom: 10px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 8px;
            border-left: 4px solid #667eea;
        }
        .translation-time {
            font-size: 0.8em;
            color: #888;
            margin-bottom: 5px;
        }
        .translation-text {
            font-size: 1.1em;
            color: #e0e0e0;
        }
        .translation-confidence {
            font-size: 0.9em;
            color: #aaa;
            margin-top: 5px;
        }
        .emotion-dropdown {
            position: relative;
            display: inline-block;
        }
        .emotion-summary {
            padding: 8px 12px;
            cursor: pointer;
            background-color: #333;
            border: 2px solid #555;
            border-radius: 6px;
            color: #e0e0e0;
            font-size: 14px;
            transition: all 0.3s ease;
        }
        .emotion-summary:hover {
            background-color: #444;
            border-color: #777;
        }
        .emotion-details {
            display: none;
            position: absolute;
            top: 100%;
            right: 0;
            margin-top: 5px;
            background: linear-gradient(145deg, #2a2a2a, #3a3a3a);
            border: 2px solid #555;
            border-radius: 12px;
            padding: 15px;
            min-width: 350px;
            box-shadow: 0 8px 16px rgba(0,0,0,0.5);
            z-index: 1000;
        }
        .emotion-details.show {
            display: block;
        }
        .emotion-details h4 {
            margin: 0 0 10px 0;
            font-size: 1em;
        }
        .emotion-detail-row {
            margin: 8px 0;
            font-size: 0.85em;
        }
    </style>
</head>
<body>
    <h1>üéØ Project Synapse - Real-Time AI Processing</h1>
    
    <div class="controls">
        <label for="camera-select">üìπ Camera:</label>
        <select id="camera-select"></select>
        <button id="switch-camera-btn">Switch Camera</button>
        <button id="test-connection-btn">Test Connection</button>
        <button id="toggle-gender-btn" title="Toggle Gender" aria-pressed="true">‚ö• Gender: ON</button>
        <button id="upload-test-video-btn" title="Upload Test Video">üìπ Test Video</button>
        <input type="file" id="test-video-input" accept="video/*" style="display: none;">
        
        <!-- Emotion Dropdown -->
        <div class="emotion-dropdown">
            <div class="emotion-summary" id="emotion-summary">
                üòê neutral | 0%
            </div>
            <div class="emotion-details" id="emotion-details">
                <h4>üé≠ Advanced Emotion Detection</h4>
                <div class="emotion-detail-row">
                    <strong>Emotion:</strong> <span id="emotion-display-detail">üòê neutral</span>
                </div>
                <div class="emotion-detail-row">
                    <strong>Confidence:</strong> <span id="emotion-confidence-detail">0%</span>
                </div>
                <div class="emotion-detail-row">
                    <strong>Valence:</strong> <span id="valence-value-detail">0.0</span> (üò¢ ‚Üê ‚Üí üòä)
                </div>
                <div class="emotion-detail-row">
                    <strong>Arousal:</strong> <span id="arousal-value-detail">0.0</span> (üò¥ ‚Üê ‚Üí ü§Ø)
                </div>
                <div class="emotion-detail-row">
                    <button id="tune-emotions-btn" style="padding: 5px 10px; width: 100%;">üéõÔ∏è Fine-tune</button>
                </div>
                <div id="emotion-probabilities-detail" style="font-size: 0.7em; color: #666; margin-top: 10px;"></div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="video-container">
            <div class="video-box">
                <h2>üì∑ Your Webcam</h2>
                <video id="webcam" width="400" height="300" autoplay muted></video>
                <div class="stats" id="input-stats">Waiting...</div>
            </div>
            
            <div class="video-box">
                <h2>ü§ñ AI Processed Feed</h2>
                <div class="media-stack">
                    <img id="processed-stream" width="400" height="300" style="background: #1a1a1a;">
                    <canvas id="overlay-canvas" width="400" height="300"></canvas>
                </div>
                <div class="stats" id="output-stats">No data</div>
            </div>
        </div>
        
        <div class="translation-container">
            <div class="translation-box">
                <h2>üí¨ Sign Language Translation</h2>
                <div id="translation-display">
                    <p style="color: #888; text-align: center; padding: 20px;">
                        Sign language translations will appear here in real-time...
                    </p>
                </div>
            </div>
        </div>
    </div>
    
    <!-- Fine-tuning Modal (hidden by default) -->
    <div id="tune-modal" style="display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); 
                                background: #2a2a2a; padding: 20px; border-radius: 10px; border: 2px solid #555; z-index: 1000;">
        <h3>üéõÔ∏è Fine-tune Emotion Detection</h3>
        <div style="margin: 10px 0;">
            <label>Happy Threshold: <input type="range" id="happy-threshold" min="0.1" max="1.0" step="0.05" value="0.5"> <span id="happy-val">0.5</span></label>
        </div>
        <div style="margin: 10px 0;">
            <label>Sad Threshold: <input type="range" id="sad-threshold" min="0.1" max="1.0" step="0.05" value="0.4"> <span id="sad-val">0.4</span></label>
        </div>
        <div style="margin: 10px 0;">
            <label>Neutral Threshold: <input type="range" id="neutral-threshold" min="0.1" max="1.0" step="0.05" value="0.3"> <span id="neutral-val">0.3</span></label>
        </div>
        <div style="margin: 15px 0;">
            <button id="apply-tune-btn">‚úÖ Apply</button>
            <button id="close-tune-btn">‚ùå Close</button>
        </div>
    </div>
    
    <p id="status" class="disconnected">Status: Initializing...</p>
    <canvas id="capture-canvas" width="400" height="300" style="display: none;"></canvas>

    <script>
        // DOM Elements
        const video = document.getElementById('webcam');
        const captureCanvas = document.getElementById('capture-canvas');
        const captureContext = captureCanvas.getContext('2d');
        const processedStream = document.getElementById('processed-stream');
        const status = document.getElementById('status');
        const cameraSelect = document.getElementById('camera-select');
        const switchBtn = document.getElementById('switch-camera-btn');
        const testBtn = document.getElementById('test-connection-btn');
        const overlayCanvas = document.getElementById('overlay-canvas');
        const overlayContext = overlayCanvas.getContext('2d');
        const inputStats = document.getElementById('input-stats');
        const outputStats = document.getElementById('output-stats');
        const emotionSummary = document.getElementById('emotion-summary');
        const emotionDetails = document.getElementById('emotion-details');
        const emotionDisplayDetail = document.getElementById('emotion-display-detail');
        const emotionConfidenceDetail = document.getElementById('emotion-confidence-detail');
        const valenceValueDetail = document.getElementById('valence-value-detail');
        const arousalValueDetail = document.getElementById('arousal-value-detail');
        const emotionProbabilitiesDetail = document.getElementById('emotion-probabilities-detail');
        const tuneEmotionsBtn = document.getElementById('tune-emotions-btn');
        const tuneModal = document.getElementById('tune-modal');
        const toggleGenderBtn = document.getElementById('toggle-gender-btn');
        const uploadTestVideoBtn = document.getElementById('upload-test-video-btn');
        const testVideoInput = document.getElementById('test-video-input');
        const translationDisplay = document.getElementById('translation-display');

        // Constants - removed unused GENDER_COLORS

        // Emotion helper function
        function getEmotionEmoji(emotion) {
            const emojis = {
                'happy': 'üòä',
                'sad': 'üò¢', 
                'angry': 'üò†',
                'surprised': 'üòÆ',
                'disgusted': 'ü§¢',
                'fearful': 'üò®',
                'neutral': 'üòê'
            };
            return emojis[emotion] || 'üòê';
        }
        
        function getEmotionColor(emotion) {
            const colors = {
                'happy': '#00ff00',
                'sad': '#4d79ff',
                'angry': '#ff4444',
                'surprised': '#ffff00',
                'disgusted': '#9933cc',
                'fearful': '#ff8800',
                'neutral': '#cccccc'
            };
            return colors[emotion] || '#cccccc';
        }
        
        let currentStream;
        let ws;
        let frameCount = 0;
        let lastFrameTime = Date.now();
        let translationHistory = [];

        function connectWebSocket() {
            console.log('üîÑ Attempting WebSocket connection...');
            status.textContent = "üîÑ Connecting to server...";
            // Use dynamic host so it works in Codespaces / remote deployments
            const protocol = (location.protocol === 'https:') ? 'wss:' : 'ws:';
            const wsUrl = protocol + '//' + location.host + '/ws';
            ws = new WebSocket(wsUrl);
            console.log('üåê WebSocket URL:', wsUrl);
            
            ws.onopen = () => {
                console.log('‚úÖ WebSocket connected');
                status.textContent = "Status: üü¢ Connected - Streaming Active";
                status.className = "connected";
                setInterval(sendFrame, 33); // ~30 FPS
            };

            ws.onmessage = (event) => {
                try {
                    const data = JSON.parse(event.data);
                    
                    // Update processed image
                    processedStream.src = "data:image/jpeg;base64," + data.image;
                    
                    // Client-side overlay not needed; server draws overlays into image now
                    overlayContext.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
                    
                    // Update stats with emotion information
                    const processingTime = data.debug || {};
                    const emotion = data.emotion || {};
                    const emotionText = emotion.emotion ? `${getEmotionEmoji(emotion.emotion)} ${emotion.emotion} (${(emotion.confidence * 100).toFixed(0)}%)` : 'No emotion';

                    // Count recognized faces (not unknown and reasonable box size)
                    const validFaces = data.faces ? data.faces.filter(face => {
                        const [startX, startY, endX, endY] = face.box;
                        const boxArea = (endX - startX) * (endY - startY);
                        const imageArea = 640 * 480;
                        return face.name !== 'Unknown' && boxArea < imageArea * 0.95 && boxArea > 1000;
                    }).length : 0;

                    outputStats.textContent = `Faces: ${data.faces?.length || 0} | Recognized: ${validFaces} | ` +
                        `MP: ${processingTime.mediapipe_ms || 0}ms | ` +
                        `Face: ${processingTime.face_recognition_ms || 0}ms | ` +
                        `Emotion: ${emotionText}`;

                    // Reflect gender toggle state in the button
                    if (data.toggles && typeof data.toggles.gender_enabled === 'boolean') {
                        const enabled = data.toggles.gender_enabled;
                        toggleGenderBtn.textContent = enabled ? '‚ö• Gender: ON' : '‚ö• Gender: OFF';
                        toggleGenderBtn.setAttribute('aria-pressed', enabled ? 'true' : 'false');
                    }
                    
                    // Update emotion display in dropdown
                    if (emotion.emotion && emotion.confidence > 0.2) {
                        const emoji = getEmotionEmoji(emotion.emotion);
                        const conf = (emotion.confidence * 100).toFixed(0);
                        
                        // Update summary
                        emotionSummary.textContent = `${emoji} ${emotion.emotion} | ${conf}%`;
                        emotionSummary.style.color = getEmotionColor(emotion.emotion);
                        
                        // Update details
                        emotionDisplayDetail.textContent = `${emoji} ${emotion.emotion}`;
                        emotionConfidenceDetail.textContent = `${conf}%`;
                        emotionDisplayDetail.style.color = getEmotionColor(emotion.emotion);
                        
                        // Update valence-arousal values
                        if (emotion.valence !== null) {
                            valenceValueDetail.textContent = emotion.valence.toFixed(2);
                            arousalValueDetail.textContent = emotion.arousal.toFixed(2);
                        }
                        
                        // Show smoothed probabilities
                        if (emotion.smoothed_probabilities) {
                            const probsText = Object.entries(emotion.smoothed_probabilities)
                                .filter(([_, prob]) => prob > 0.1)
                                .map(([emotion, prob]) => `${emotion}: ${(prob * 100).toFixed(0)}%`)
                                .join(' | ');
                            emotionProbabilitiesDetail.textContent = probsText;
                        }
                    } else {
                        emotionSummary.textContent = 'üòê neutral | 0%';
                        emotionSummary.style.color = '#ccc';
                        emotionDisplayDetail.textContent = 'üòê neutral';
                        emotionConfidenceDetail.textContent = '0%';
                        emotionDisplayDetail.style.color = '#ccc';
                        valenceValueDetail.textContent = '0.0';
                        arousalValueDetail.textContent = '0.0';
                    }
                    
                    // Update sign language translation display
                    if (data.sign && data.sign.predicted_sign && data.sign.predicted_sign !== 'No Model' && 
                        data.sign.predicted_sign !== 'Collecting...' && data.sign.predicted_sign !== 'Uncertain' &&
                        data.sign.confidence > 0.5) {
                        addTranslation(data.sign.predicted_sign, data.sign.confidence);
                    }
                    
                } catch (e) {
                    console.error("Error processing message:", e);
                }
            };

            ws.onclose = (event) => {
                console.log('‚ùå WebSocket closed:', event.code, event.reason);
                status.textContent = "Status: üî¥ Disconnected - Reconnecting...";
                status.className = "disconnected";
                setTimeout(connectWebSocket, 3000); // Auto-reconnect
            };

            ws.onerror = (error) => {
                console.error('‚ùå WebSocket error:', error);
                status.textContent = "Status: ‚ö†Ô∏è Connection Error";
                status.className = "error";
            };
        }

        function sendFrame() {
            if (ws && ws.readyState === WebSocket.OPEN && video.videoWidth > 0) {
                captureContext.drawImage(video, 0, 0, 400, 300);
                captureCanvas.toBlob((blob) => {
                    if (blob) {
                        ws.send(blob);
                        
                        // Update input stats
                        frameCount++;
                        const now = Date.now();
                        if (now - lastFrameTime > 1000) {
                            const fps = Math.round(frameCount * 1000 / (now - lastFrameTime));
                            inputStats.textContent = `${fps} FPS | ${Math.round(blob.size/1024)}KB`;
                            frameCount = 0;
                            lastFrameTime = now;
                        }
                    }
                }, 'image/jpeg', 0.8);
            } else if (ws && ws.readyState === WebSocket.OPEN && video.videoWidth === 0) {
                // Fallback synthetic frame when webcam not available (e.g., remote Codespace)
                frameCount++;
                const t = Date.now() / 1000;
                captureContext.fillStyle = '#111';
                captureContext.fillRect(0,0,400,300);
                // Animated gradient bars
                for (let i=0;i<10;i++) {
                    const y = (i * 30) + ((t*30) % 30);
                    const hue = (i*36 + t*40) % 360;
                    captureContext.fillStyle = `hsl(${hue},70%,50%)`;
                    captureContext.fillRect(0, y % 300, 400, 15);
                }
                captureContext.fillStyle = '#fff';
                captureContext.font = '16px Segoe UI';
                captureContext.fillText('No webcam detected - sending synthetic frames', 10, 30);
                captureContext.fillText('Grant camera permission or attach a camera.', 10, 50);
                captureContext.fillText('Frame #'+frameCount, 10, 70);
                captureCanvas.toBlob(blob => {
                    if (blob) {
                        ws.send(blob);
                        if (frameCount % 30 === 0) {
                            inputStats.textContent = `Synthetic ${Math.round(frameCount / t)} FPS | ${Math.round(blob.size/1024)}KB`;
                        }
                    }
                }, 'image/jpeg', 0.7);
            }
        }

        async function getCameras() {
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');
                
                cameraSelect.innerHTML = '';
                videoDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Camera ${index + 1}`;
                    cameraSelect.appendChild(option);
                });
                
                console.log(`Found ${videoDevices.length} cameras`);
            } catch (err) {
                console.error("Error enumerating devices:", err);
            }
        }

        async function setupWebcam(deviceId) {
            if (currentStream) {
                currentStream.getTracks().forEach(track => track.stop());
            }
            
            const constraints = {
                video: {
                    deviceId: deviceId ? { exact: deviceId } : undefined,
                    width: { ideal: 400 },
                    height: { ideal: 300 },
                    frameRate: { ideal: 30 }
                }
            };
            
            try {
                console.log('üîÑ Requesting camera access...');
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                console.log('‚úÖ Camera access granted');
                
                video.srcObject = stream;
                currentStream = stream;
                
                video.onloadedmetadata = () => {
                    console.log(`‚úÖ Video loaded: ${video.videoWidth}x${video.videoHeight}`);
                };
                
            } catch (err) {
                console.error('‚ùå Camera error:', err);
                status.textContent = `‚ùå Camera Error: ${err.message}`;
                status.className = "error";
                
                // Show more specific error messages
                if (err.name === 'NotAllowedError') {
                    status.textContent = "‚ùå Camera permission denied. Please allow camera access.";
                } else if (err.name === 'NotFoundError') {
                    status.textContent = "‚ùå No camera found. Please connect a camera.";
                } else if (err.name === 'NotReadableError') {
                    status.textContent = "‚ùå Camera is already in use by another application.";
                }
            }
        }

        // Event Listeners
        switchBtn.addEventListener('click', () => {
            setupWebcam(cameraSelect.value);
        });

        testBtn.addEventListener('click', async () => {
            try {
                const response = await fetch('/health');
                const data = await response.json();
                alert(`Server Health: ${JSON.stringify(data, null, 2)}`);
            } catch (e) {
                alert('Server connection failed!');
            }
        });

        // Gender toggle
        toggleGenderBtn.addEventListener('click', async () => {
            try {
                const res = await fetch('/gender/toggle', { method: 'POST' });
                const data = await res.json();
                const enabled = !!data.enabled;
                toggleGenderBtn.textContent = enabled ? '‚ö• Gender: ON' : '‚ö• Gender: OFF';
                toggleGenderBtn.setAttribute('aria-pressed', enabled ? 'true' : 'false');
            } catch (e) {
                console.error('Failed to toggle gender:', e);
            }
        });

        // Emotion dropdown toggle
        emotionSummary.addEventListener('click', () => {
            emotionDetails.classList.toggle('show');
        });

        // Close emotion dropdown when clicking outside
        document.addEventListener('click', (e) => {
            if (!emotionSummary.contains(e.target) && !emotionDetails.contains(e.target)) {
                emotionDetails.classList.remove('show');
            }
        });

        // Test video upload
        uploadTestVideoBtn.addEventListener('click', () => {
            testVideoInput.click();
        });

        testVideoInput.addEventListener('change', async (e) => {
            const file = e.target.files[0];
            if (!file) return;
            
            try {
                status.textContent = `‚è≥ Uploading video: ${file.name}...`;
                status.className = 'connected';
                
                // Upload video
                const formData = new FormData();
                formData.append('file', file);
                
                const uploadResponse = await fetch('/video/upload', {
                    method: 'POST',
                    body: formData
                });
                
                const uploadResult = await uploadResponse.json();
                
                if (uploadResult.status === 'success') {
                    status.textContent = `‚úÖ Video uploaded! Processing...`;
                    
                    // Process video
                    const processResponse = await fetch(`/video/process/${uploadResult.filename}`, {
                        method: 'POST'
                    });
                    
                    const processResult = await processResponse.json();
                    
                    if (processResult.status === 'success') {
                        displayVideoResults(processResult);
                        status.textContent = `‚úÖ Video processed successfully!`;
                    } else {
                        alert(`Processing failed: ${processResult.message}`);
                        status.textContent = `‚ùå Processing failed`;
                    }
                } else {
                    alert(`Upload failed: ${uploadResult.message}`);
                    status.textContent = `‚ùå Upload failed`;
                }
                
            } catch (error) {
                console.error('Video upload/process error:', error);
                alert(`Error: ${error.message}`);
                status.textContent = `‚ùå Error: ${error.message}`;
                status.className = 'error';
            }
            
            // Reset input
            e.target.value = '';
        });

        // Display video processing results
        function displayVideoResults(result) {
            const summary = result.summary || {};
            const videoInfo = result.video_info || {};
            
            let html = `
                <div style="background: rgba(0,255,0,0.1); padding: 15px; border-radius: 8px; margin-bottom: 15px;">
                    <h3 style="margin: 0 0 10px 0;">üìπ Test Video Results</h3>
                    <p><strong>Duration:</strong> ${videoInfo.duration?.toFixed(1)}s</p>
                    <p><strong>Frames Processed:</strong> ${videoInfo.processed_frames} / ${videoInfo.total_frames}</p>
                    <p><strong>Predictions:</strong> ${summary.total_predictions}</p>
                    <p><strong>Unique Signs:</strong> ${summary.unique_signs}</p>
                </div>
            `;
            
            if (summary.signs_detected) {
                html += '<h4>Detected Signs:</h4>';
                for (const [sign, data] of Object.entries(summary.signs_detected)) {
                    html += `
                        <div class="translation-item">
                            <div class="translation-text">ü§ü ${sign}</div>
                            <div class="translation-confidence">
                                Count: ${data.count} | Avg Confidence: ${(data.avg_confidence * 100).toFixed(0)}%
                            </div>
                        </div>
                    `;
                }
            }
            
            translationDisplay.innerHTML = html;
        }

        // Translation display function
        function addTranslation(signText, confidence) {
            const now = new Date();
            const timeStr = now.toLocaleTimeString();
            
            // Avoid duplicate consecutive translations
            if (translationHistory.length > 0) {
                const last = translationHistory[translationHistory.length - 1];
                if (last.text === signText && (now - last.timestamp) < 2000) {
                    return; // Skip duplicate within 2 seconds
                }
            }
            
            const translation = {
                text: signText,
                confidence: confidence,
                timestamp: now,
                timeStr: timeStr
            };
            
            translationHistory.push(translation);
            
            // Keep only last 50 translations
            if (translationHistory.length > 50) {
                translationHistory = translationHistory.slice(-50);
            }
            
            // Update display
            updateTranslationDisplay();
            
            // Text-to-speech (if available)
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(signText);
                utterance.rate = 0.9;
                utterance.pitch = 1.0;
                utterance.volume = 0.8;
                window.speechSynthesis.speak(utterance);
            }
        }

        function updateTranslationDisplay() {
            if (translationHistory.length === 0) {
                translationDisplay.innerHTML = `
                    <p style="color: #888; text-align: center; padding: 20px;">
                        Sign language translations will appear here in real-time...
                    </p>
                `;
                return;
            }
            
            let html = '';
            for (let i = translationHistory.length - 1; i >= 0; i--) {
                const t = translationHistory[i];
                html += `
                    <div class="translation-item">
                        <div class="translation-time">${t.timeStr}</div>
                        <div class="translation-text">ü§ü ${t.text}</div>
                        <div class="translation-confidence">Confidence: ${(t.confidence * 100).toFixed(0)}%</div>
                    </div>
                `;
            }
            translationDisplay.innerHTML = html;
            
            // Auto-scroll to latest translation
            translationDisplay.scrollTop = 0;
        }

        // Fine-tuning functionality
        tuneEmotionsBtn.addEventListener('click', () => {
            tuneModal.style.display = 'block';
        });

        document.getElementById('close-tune-btn').addEventListener('click', () => {
            tuneModal.style.display = 'none';
        });

        // Update threshold displays
        document.getElementById('happy-threshold').addEventListener('input', (e) => {
            document.getElementById('happy-val').textContent = e.target.value;
        });
        
        document.getElementById('sad-threshold').addEventListener('input', (e) => {
            document.getElementById('sad-val').textContent = e.target.value;
        });
        
        document.getElementById('neutral-threshold').addEventListener('input', (e) => {
            document.getElementById('neutral-val').textContent = e.target.value;
        });

        // Apply fine-tuning
        document.getElementById('apply-tune-btn').addEventListener('click', async () => {
            const tuneParams = {
                emotion_thresholds: {
                    happy: parseFloat(document.getElementById('happy-threshold').value),
                    sad: parseFloat(document.getElementById('sad-threshold').value),
                    neutral: parseFloat(document.getElementById('neutral-threshold').value)
                }
            };

            try {
                const response = await fetch('/emotion/tune', {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(tuneParams)
                });
                
                const result = await response.json();
                
                if (result.success) {
                    alert('‚úÖ Emotion thresholds updated successfully!');
                } else {
                    alert('‚ùå Failed to update thresholds: ' + result.error);
                }
                
            } catch (error) {
                alert('‚ùå Network error: ' + error.message);
            }
            
            tuneModal.style.display = 'none';
        });

        // Initialize
        async function init() {
            try {
                console.log('üîÑ Starting initialization...');
                status.textContent = "üîÑ Getting cameras...";
                
                await getCameras();
                console.log('‚úÖ Cameras enumerated');
                
                status.textContent = "üîÑ Setting up webcam...";
                await setupWebcam();
                console.log('‚úÖ Webcam setup complete');
                
                status.textContent = "üîÑ Connecting WebSocket...";
                connectWebSocket();
                console.log('‚úÖ WebSocket connection initiated');
                
            } catch (error) {
                console.error('‚ùå Initialization error:', error);
                status.textContent = `‚ùå Error: ${error.message}`;
                status.className = "error";
            }
        }

        // Add global error handler
        window.addEventListener('error', function(e) {
            console.error('‚ùå Global error:', e.error);
            status.textContent = `‚ùå JavaScript Error: ${e.error.message}`;
            status.className = "error";
        });

        window.addEventListener('unhandledrejection', function(e) {
            console.error('‚ùå Unhandled promise rejection:', e.reason);
            status.textContent = `‚ùå Promise Error: ${e.reason}`;
            status.className = "error";
        });

        console.log('üöÄ Starting MajorSignL frontend...');
        init();
    </script>
</body>
</html>
